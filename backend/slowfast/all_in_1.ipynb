{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc488ca",
   "metadata": {},
   "source": [
    "Is made for google collab, change directories and eliminate torch.devices in local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d3be3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.io as io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07579a5a",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6773611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para resultados reproducibles\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "#print(f\"Usando: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655823bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Rutas del proyecto - AJUSTAR SEGÚN TU ESTRUCTURA\n",
    "BASE_PATH = \"/content/drive/MyDrive/slowfast/videos\"\n",
    "VIDEOS_PATH = BASE_PATH  # Si tus videos están en otra ubicación, ajusta esta ruta\n",
    "METADATA_PATH = \"/content/drive/MyDrive/slowfast/metadata.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316e609",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar metadatos\n",
    "def load_metadata(metadata_path):\n",
    "    try:\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Convertir a DataFrame para facilitar el procesamiento\n",
    "        df = pd.DataFrame(metadata)\n",
    "        \n",
    "        # Si no hay una columna \"output_path\", crear una basada en output_filename\n",
    "        if 'output_path' not in df.columns and 'output_filename' in df.columns:\n",
    "            # Extraer directorio del primer elemento para ver el patrón\n",
    "            if len(df) > 0 and 'stroke_type' in df.columns and 'shot_variant' in df.columns:\n",
    "                # Crear rutas basadas en la estructura de directorios mencionada\n",
    "                df['output_path'] = df.apply(\n",
    "                    lambda row: os.path.join('videos', row['stroke_type'], row['shot_variant'], row['output_filename']), \n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                # Si no hay suficiente información, usar solo el nombre del archivo\n",
    "                df['output_path'] = df['output_filename'].apply(lambda x: os.path.join('videos', x))\n",
    "        \n",
    "        # Añadir columna para la ruta completa\n",
    "        df['full_path'] = df['output_path']\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar metadatos: {e}\")\n",
    "        # Crear un DataFrame vacío con las columnas necesarias como fallback\n",
    "        return pd.DataFrame(columns=['input_video', 'stroke_type', 'shot_variant', 'hand_style', \n",
    "                                    'output_filename', 'output_path', 'full_path'])\n",
    "\n",
    "# Crear mapeos para las etiquetas (convertir de texto a índices)\n",
    "def create_label_mappings(df):\n",
    "    # Mapeo para stroke_type (tarea 1)\n",
    "    stroke_types = sorted(df['stroke_type'].unique())\n",
    "    stroke_type_to_idx = {stroke: idx for idx, stroke in enumerate(stroke_types)}\n",
    "    \n",
    "    # Mapeo para shot_variant (tarea 2)\n",
    "    shot_variants = sorted(df['shot_variant'].unique())\n",
    "    shot_variant_to_idx = {variant: idx for idx, variant in enumerate(shot_variants)}\n",
    "    \n",
    "    # Mapeo para hand_style (tarea 3)\n",
    "    hand_styles = sorted(df['hand_style'].unique().tolist())\n",
    "    \n",
    "    # Asegurarse de que no_hand_style esté al final\n",
    "    if 'no_hand_style' in hand_styles:\n",
    "        hand_styles.remove('no_hand_style')\n",
    "        hand_styles.append('no_hand_style')  # Mover al final\n",
    "    \n",
    "    # Si sólo hay 1 clase (todos 'no_hand_style' por ejemplo)\n",
    "    # Crear una clase ficticia para evitar errores\n",
    "    if len(hand_styles) < 2:\n",
    "        print(f\"¡Advertencia! Solo hay {len(hand_styles)} estilo(s) de mano: {hand_styles}\")\n",
    "        print(\"Agregando una clase ficticia para evitar errores.\")\n",
    "        if 'one' not in hand_styles:\n",
    "            hand_styles.insert(0, 'one')\n",
    "        elif 'two' not in hand_styles:\n",
    "            hand_styles.insert(0, 'two')\n",
    "    \n",
    "    hand_style_to_idx = {style: idx for idx, style in enumerate(hand_styles)}\n",
    "    \n",
    "    return {\n",
    "        'stroke_type': stroke_type_to_idx,\n",
    "        'shot_variant': shot_variant_to_idx,\n",
    "        'hand_style': hand_style_to_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614658d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TennisVideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, label_mappings, transform=None, clip_len=32, \n",
    "                 skip_rate=2, slow_pathway_size=8, fast_pathway_size=32, demo_mode=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.label_mappings = label_mappings\n",
    "        self.transform = transform\n",
    "        self.clip_len = clip_len\n",
    "        self.skip_rate = skip_rate\n",
    "        self.slow_pathway_size = slow_pathway_size\n",
    "        self.fast_pathway_size = fast_pathway_size\n",
    "        self.demo_mode = demo_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Obtener información del video\n",
    "        video_info = self.dataframe.iloc[index]\n",
    "        video_path = os.path.join(BASE_PATH, video_info['full_path'])\n",
    "        \n",
    "        # Obtener etiquetas\n",
    "        # Asegurarse de que existan en los mapeos\n",
    "        try:\n",
    "            stroke_type_label = self.label_mappings['stroke_type'][video_info['stroke_type']]\n",
    "        except KeyError:\n",
    "            print(f\"¡Error! Tipo de golpe desconocido: {video_info['stroke_type']}\")\n",
    "            stroke_type_label = 0  # Usar la primera clase como fallback\n",
    "            \n",
    "        try:\n",
    "            shot_variant_label = self.label_mappings['shot_variant'][video_info['shot_variant']]\n",
    "        except KeyError:\n",
    "            print(f\"¡Error! Variante de golpe desconocida: {video_info['shot_variant']}\")\n",
    "            shot_variant_label = 0  # Usar la primera clase como fallback\n",
    "            \n",
    "        hand_style = video_info['hand_style']\n",
    "        try:\n",
    "            hand_style_label = self.label_mappings['hand_style'][hand_style]\n",
    "        except KeyError:\n",
    "            print(f\"¡Error! Estilo de mano desconocido: {hand_style}\")\n",
    "            hand_style_label = 0  # Usar la primera clase como fallback\n",
    "            \n",
    "        # Modo de demostración - generar tensores aleatorios en lugar de cargar videos\n",
    "        if self.demo_mode or not os.path.exists(video_path):\n",
    "            # Generar datos sintéticos para demostración\n",
    "            # Fast pathway - más frames\n",
    "            fast_pathway = torch.rand(3, self.fast_pathway_size, 112, 112)\n",
    "            \n",
    "            # Slow pathway - menos frames\n",
    "            slow_pathway = torch.rand(3, self.slow_pathway_size, 112, 112)\n",
    "            \n",
    "            return {\n",
    "                'slow_pathway': slow_pathway, \n",
    "                'fast_pathway': fast_pathway,\n",
    "                'stroke_type': stroke_type_label,\n",
    "                'shot_variant': shot_variant_label, \n",
    "                'hand_style': hand_style_label\n",
    "            }\n",
    "        \n",
    "        # Cargar y preprocesar el video real (si existe) usando OpenCV\n",
    "        try:\n",
    "            # Abrir el video con OpenCV\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise Exception(f\"No se pudo abrir el video: {video_path}\")\n",
    "            \n",
    "            # Obtener información del video\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            if total_frames <= 0:\n",
    "                raise Exception(f\"El video no tiene frames o frames inválidos: {video_path}\")\n",
    "            \n",
    "            # Seleccionar frames uniformemente distribuidos para cada pathway\n",
    "            fast_indices = np.linspace(0, total_frames - 1, self.fast_pathway_size, dtype=int)\n",
    "            slow_indices = np.linspace(0, total_frames - 1, self.slow_pathway_size, dtype=int)\n",
    "            \n",
    "            # Leer frames para el fast pathway\n",
    "            fast_frames = []\n",
    "            for idx in fast_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    raise Exception(f\"Error leyendo frame {idx} del video {video_path}\")\n",
    "                # Convertir de BGR a RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Redimensionar a 112x112\n",
    "                frame = cv2.resize(frame, (112, 112))\n",
    "                fast_frames.append(frame)\n",
    "            \n",
    "            # Reiniciar para leer frames para el slow pathway\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            slow_frames = []\n",
    "            for idx in slow_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    raise Exception(f\"Error leyendo frame {idx} del video {video_path}\")\n",
    "                # Convertir de BGR a RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Redimensionar a 112x112\n",
    "                frame = cv2.resize(frame, (112, 112))\n",
    "                slow_frames.append(frame)\n",
    "            \n",
    "            # Cerrar el video\n",
    "            cap.release()\n",
    "            \n",
    "            # Convertir listas a tensores\n",
    "            fast_pathway = torch.from_numpy(np.array(fast_frames)).permute(3, 0, 1, 2).float() / 255.0\n",
    "            slow_pathway = torch.from_numpy(np.array(slow_frames)).permute(3, 0, 1, 2).float() / 255.0\n",
    "            \n",
    "            # Aplicar transformaciones si hay\n",
    "            if self.transform:\n",
    "                # Para normalización, aplicar a cada frame\n",
    "                for t in range(fast_pathway.shape[1]):\n",
    "                    fast_pathway[:, t] = self.transform(fast_pathway[:, t])\n",
    "                for t in range(slow_pathway.shape[1]):\n",
    "                    slow_pathway[:, t] = self.transform(slow_pathway[:, t])\n",
    "            \n",
    "            return {\n",
    "                'slow_pathway': slow_pathway, \n",
    "                'fast_pathway': fast_pathway,\n",
    "                'stroke_type': stroke_type_label,\n",
    "                'shot_variant': shot_variant_label, \n",
    "                'hand_style': hand_style_label\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando video {video_path}: {e}\")\n",
    "            # Retornar un tensor aleatorio con las dimensiones correctas\n",
    "            return {\n",
    "                'slow_pathway': torch.rand(3, self.slow_pathway_size, 112, 112),\n",
    "                'fast_pathway': torch.rand(3, self.fast_pathway_size, 112, 112),\n",
    "                'stroke_type': stroke_type_label,\n",
    "                'shot_variant': shot_variant_label, \n",
    "                'hand_style': hand_style_label\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136fca1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=(1, stride, stride), padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=(1, stride, stride)),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SlowFastNetwork(nn.Module):\n",
    "    def __init__(self, slow_channels=3, fast_channels=3, \n",
    "                 num_stroke_classes=4, num_variant_classes=7, num_hand_classes=3):\n",
    "        super(SlowFastNetwork, self).__init__()\n",
    "        \n",
    "        # Configuración del pathway lento (características más espaciales)\n",
    "        self.slow_pathway = nn.Sequential(\n",
    "            nn.Conv3d(slow_channels, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "            \n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 128, stride=2),\n",
    "            ResBlock(128, 256, stride=2),\n",
    "            ResBlock(256, 512, stride=2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        )\n",
    "        \n",
    "        # Configuración del pathway rápido (características más temporales)\n",
    "        self.fast_pathway = nn.Sequential(\n",
    "            nn.Conv3d(fast_channels, 32, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "            \n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 64, stride=2),\n",
    "            ResBlock(64, 128, stride=2),\n",
    "            ResBlock(128, 256, stride=2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        )\n",
    "        \n",
    "        # Fusion y clasificación\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Cabezas multi-tarea\n",
    "        self.stroke_classifier = nn.Linear(512, num_stroke_classes)\n",
    "        self.variant_classifier = nn.Linear(512, num_variant_classes)\n",
    "        self.hand_classifier = nn.Linear(512, num_hand_classes)\n",
    "        \n",
    "    def forward(self, slow_input, fast_input):\n",
    "        # Procesar cada pathway\n",
    "        slow_features = self.slow_pathway(slow_input)\n",
    "        fast_features = self.fast_pathway(fast_input)\n",
    "        \n",
    "        # Aplanar las características\n",
    "        slow_features = slow_features.view(slow_features.size(0), -1)\n",
    "        fast_features = fast_features.view(fast_features.size(0), -1)\n",
    "        \n",
    "        # Fusionar características\n",
    "        fused_features = torch.cat([slow_features, fast_features], dim=1)\n",
    "        fused_features = self.fusion(fused_features)\n",
    "        \n",
    "        # Clasificación multi-tarea\n",
    "        stroke_preds = self.stroke_classifier(fused_features)\n",
    "        variant_preds = self.variant_classifier(fused_features)\n",
    "        hand_preds = self.hand_classifier(fused_features)\n",
    "        \n",
    "        return stroke_preds, variant_preds, hand_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee96a02",
   "metadata": {},
   "source": [
    "### Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de pérdida multi-tarea\n",
    "def compute_loss(stroke_preds, variant_preds, hand_preds, \n",
    "                stroke_targets, variant_targets, hand_targets):\n",
    "    # Usar CrossEntropy para cada tarea\n",
    "    stroke_loss = F.cross_entropy(stroke_preds, stroke_targets)\n",
    "    variant_loss = F.cross_entropy(variant_preds, variant_targets)\n",
    "    hand_loss = F.cross_entropy(hand_preds, hand_targets)\n",
    "    \n",
    "    # Combinar pérdidas (se pueden ajustar los pesos)\n",
    "    total_loss = stroke_loss + variant_loss + hand_loss\n",
    "    \n",
    "    return {\n",
    "        'total': total_loss,\n",
    "        'stroke': stroke_loss.item(),\n",
    "        'variant': variant_loss.item(),\n",
    "        'hand': hand_loss.item()\n",
    "    }\n",
    "\n",
    "# Evaluar el modelo\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Métricas\n",
    "    total_loss = 0\n",
    "    stroke_correct = 0\n",
    "    variant_correct = 0\n",
    "    hand_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Mover los datos al dispositivo\n",
    "            slow_pathway = batch['slow_pathway'].to(device)\n",
    "            fast_pathway = batch['fast_pathway'].to(device)\n",
    "            stroke_targets = batch['stroke_type'].to(device)\n",
    "            variant_targets = batch['shot_variant'].to(device)\n",
    "            hand_targets = batch['hand_style'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            stroke_preds, variant_preds, hand_preds = model(slow_pathway, fast_pathway)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss_dict = compute_loss(stroke_preds, variant_preds, hand_preds,\n",
    "                                    stroke_targets, variant_targets, hand_targets)\n",
    "            total_loss += loss_dict['total'].item()\n",
    "            \n",
    "            # Calcular precisión\n",
    "            _, stroke_pred_idx = torch.max(stroke_preds, dim=1)\n",
    "            _, variant_pred_idx = torch.max(variant_preds, dim=1)\n",
    "            _, hand_pred_idx = torch.max(hand_preds, dim=1)\n",
    "            \n",
    "            stroke_correct += (stroke_pred_idx == stroke_targets).sum().item()\n",
    "            variant_correct += (variant_pred_idx == variant_targets).sum().item()\n",
    "            hand_correct += (hand_pred_idx == hand_targets).sum().item()\n",
    "            \n",
    "            total_samples += stroke_targets.size(0)\n",
    "    \n",
    "    # Calcular métricas promedio\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    stroke_acc = stroke_correct / total_samples\n",
    "    variant_acc = variant_correct / total_samples\n",
    "    hand_acc = hand_correct / total_samples\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'stroke_acc': stroke_acc,\n",
    "        'variant_acc': variant_acc,\n",
    "        'hand_acc': hand_acc,\n",
    "        'avg_acc': (stroke_acc + variant_acc + hand_acc) / 3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd88bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función principal de entrenamiento\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, \n",
    "                num_epochs=10, save_path='model_checkpoints'):\n",
    "    \n",
    "    # Crear directorio para guardar checkpoints\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Listas para almacenar métricas\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Metrics for training\n",
    "        train_stroke_correct = 0\n",
    "        train_variant_correct = 0\n",
    "        train_hand_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Mover los datos al dispositivo\n",
    "            slow_pathway = batch['slow_pathway'].to(device)\n",
    "            fast_pathway = batch['fast_pathway'].to(device)\n",
    "            stroke_targets = batch['stroke_type'].to(device)\n",
    "            variant_targets = batch['shot_variant'].to(device)\n",
    "            hand_targets = batch['hand_style'].to(device)\n",
    "            \n",
    "            # Reiniciar gradientes\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            stroke_preds, variant_preds, hand_preds = model(slow_pathway, fast_pathway)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss_dict = compute_loss(stroke_preds, variant_preds, hand_preds,\n",
    "                                   stroke_targets, variant_targets, hand_targets)\n",
    "            \n",
    "            # Backward pass y optimización\n",
    "            loss_dict['total'].backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Actualizar pérdida total\n",
    "            train_loss += loss_dict['total'].item()\n",
    "            \n",
    "            # Calcular métricas de precisión\n",
    "            _, stroke_pred_idx = torch.max(stroke_preds, dim=1)\n",
    "            _, variant_pred_idx = torch.max(variant_preds, dim=1)\n",
    "            _, hand_pred_idx = torch.max(hand_preds, dim=1)\n",
    "            \n",
    "            train_stroke_correct += (stroke_pred_idx == stroke_targets).sum().item()\n",
    "            train_variant_correct += (variant_pred_idx == variant_targets).sum().item()\n",
    "            train_hand_correct += (hand_pred_idx == hand_targets).sum().item()\n",
    "            train_total += stroke_targets.size(0)\n",
    "            \n",
    "            # Actualizar la barra de progreso\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss_dict['total'].item(),\n",
    "                'stroke_loss': loss_dict['stroke'],\n",
    "                'variant_loss': loss_dict['variant'],\n",
    "                'hand_loss': loss_dict['hand']\n",
    "            })\n",
    "        \n",
    "        # Calcular métricas de entrenamiento\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_stroke_acc = train_stroke_correct / train_total\n",
    "        train_variant_acc = train_variant_correct / train_total\n",
    "        train_hand_acc = train_hand_correct / train_total\n",
    "        train_avg_acc = (train_stroke_acc + train_variant_acc + train_hand_acc) / 3\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accs.append(train_avg_acc)\n",
    "        \n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_accs.append(val_metrics['avg_acc'])\n",
    "        \n",
    "        # Imprimir métricas del epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_avg_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['avg_acc']:.4f}\")\n",
    "        print(f\"  Train - Stroke: {train_stroke_acc:.4f}, Variant: {train_variant_acc:.4f}, Hand: {train_hand_acc:.4f}\")\n",
    "        print(f\"  Val - Stroke: {val_metrics['stroke_acc']:.4f}, Variant: {val_metrics['variant_acc']:.4f}, Hand: {val_metrics['hand_acc']:.4f}\")\n",
    "        \n",
    "        # Guardar el mejor modelo\n",
    "        if val_metrics['avg_acc'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['avg_acc']\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_metrics['avg_acc'],\n",
    "                'val_loss': val_metrics['loss']\n",
    "            }, os.path.join(save_path, 'best_model.pth'))\n",
    "            print(f\"  Nuevo mejor modelo guardado con precisión: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Guardar el modelo final\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_accuracy': val_metrics['avg_acc'],\n",
    "        'val_loss': val_metrics['loss']\n",
    "    }, os.path.join(save_path, 'final_model.pth'))\n",
    "    \n",
    "    # Visualizar curvas de entrenamiento\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs. Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e0b75",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3efccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Cargando metadatos...\")\n",
    "    df = load_metadata(METADATA_PATH)\n",
    "    original_len = len(df)\n",
    "    print(f\"Total de videos en metadatos: {original_len}\")\n",
    "\n",
    "    # SIEMPRE FORZAR MODO DEMO = True para usar menos recursos\n",
    "    demo_mode = True  # CAMBIAR AQUÍ: Forzar modo demo\n",
    "    \n",
    "    if original_len == 0 or demo_mode:  # CAMBIO: Usar datos sintéticos siempre en modo demo\n",
    "        print(\"\\nCreando datos de ejemplo para demostración...\")\n",
    "\n",
    "        # Crear datos sintéticos para demostración\n",
    "        example_data = []\n",
    "        for i in range(10):  # CAMBIO: Reducir a 10 ejemplos (original era 30)\n",
    "            stroke_type = random.choice(['Forehand', 'Backhand'])  # CAMBIO: Solo usar 2 tipos para minimizar\n",
    "            \n",
    "            if stroke_type == 'Forehand':\n",
    "                variant = 'Topspin'  # CAMBIO: Simplificar a solo una variante\n",
    "                hand = 'no_hand_style'\n",
    "            else:  # Backhand\n",
    "                variant = 'Topspin (1H)'  # CAMBIO: Simplificar a solo una variante\n",
    "                hand = 'one'\n",
    "                \n",
    "            example_data.append({\n",
    "                'input_video': 'example.mp4',\n",
    "                'stroke_type': stroke_type,\n",
    "                'shot_variant': variant,\n",
    "                'hand_style': hand,\n",
    "                'output_filename': f'example_{i}.mp4',\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(example_data)\n",
    "        df['output_path'] = df.apply(\n",
    "            lambda row: os.path.join('videos', row['stroke_type'], row['shot_variant'], row['output_filename']), \n",
    "            axis=1\n",
    "        )\n",
    "        df['full_path'] = df['output_path']\n",
    "        print(f\"Datos de ejemplo creados: {len(df)} entradas\")\n",
    "        valid_videos = list(range(len(df)))\n",
    "    else:\n",
    "        # Código para verificar videos existentes (no se usará en modo demo)\n",
    "        valid_videos = []\n",
    "        print(\"\\nVerificando que los videos existan...\")\n",
    "        # ... [CÓDIGO ORIGINAL DE VERIFICACIÓN] ...\n",
    "    \n",
    "    # Filtrar solo los videos que existen o usar todos si son datos de ejemplo\n",
    "    if len(valid_videos) > 0:\n",
    "        df = df.loc[valid_videos].reset_index(drop=True)\n",
    "        print(f\"Videos encontrados: {len(df)}/{original_len}\")\n",
    "    else:\n",
    "        print(\"\\n¡ADVERTENCIA! No se encontró ningún video. Usando modo demo con datos sintéticos.\")\n",
    "\n",
    "    # Creación de mapeos de etiquetas\n",
    "    print(\"\\nDistribución de Stroke Types:\")\n",
    "    print(df['stroke_type'].value_counts())\n",
    "    \n",
    "    print(\"\\nDistribución de Shot Variants:\")\n",
    "    print(df['shot_variant'].value_counts())\n",
    "    \n",
    "    print(\"\\nDistribución de Hand Styles:\")\n",
    "    print(df['hand_style'].value_counts())\n",
    "    \n",
    "    # Crear mapeos\n",
    "    label_mappings = create_label_mappings(df)\n",
    "    print(\"\\nClases:\")\n",
    "    for task, mapping in label_mappings.items():\n",
    "        print(f\"  {task}: {len(mapping)} clases\")\n",
    "        if len(mapping) > 0:\n",
    "            print(f\"  - Ejemplos: {list(mapping.keys())[:min(3, len(mapping))]}\")\n",
    "    \n",
    "    # Verificar si tenemos suficientes datos para entrenar\n",
    "    if len(df) == 0 or len(label_mappings['stroke_type']) == 0:\n",
    "        print(\"\\n¡ERROR! No hay suficientes datos o clases para entrenar.\")\n",
    "        return\n",
    "    \n",
    "    # CAMBIO: Simplificar división train/val en modo demo\n",
    "    # Usar 80% de los datos sintéticos para train y 20% para val\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size].reset_index(drop=True)\n",
    "    val_df = df.iloc[train_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nConjunto de entrenamiento: {len(train_df)} videos\")\n",
    "    print(f\"Conjunto de validación: {len(val_df)} videos\")\n",
    "    \n",
    "    # CAMBIO: Simplificar transformaciones\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((56, 56)),  # CAMBIO: Reducir resolución a la mitad (original 112,112)\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Crear datasets - ELIMINADO el parámetro num_frames que causaba el error\n",
    "    train_dataset = TennisVideoDataset(\n",
    "        train_df, \n",
    "        label_mappings, \n",
    "        transform=transform, \n",
    "        demo_mode=True  # CAMBIO: Forzar demo_mode=True\n",
    "    )\n",
    "    val_dataset = TennisVideoDataset(\n",
    "        val_df, \n",
    "        label_mappings, \n",
    "        transform=transform, \n",
    "        demo_mode=True  # CAMBIO: Forzar demo_mode=True\n",
    "    )\n",
    "    \n",
    "    # CAMBIO: Reducir batch_size y workers\n",
    "    batch_size = 1  # Batch mínimo\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # CAMBIO: Forzar CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # CAMBIO: Reducir complejidad del modelo\n",
    "    num_stroke_classes = len(label_mappings['stroke_type'])\n",
    "    num_variant_classes = len(label_mappings['shot_variant'])\n",
    "    num_hand_classes = len(label_mappings['hand_style'])\n",
    "    \n",
    "    print(f\"\\nConfiguración del modelo:\")\n",
    "    print(f\"  Clases de golpe (stroke): {num_stroke_classes}\")\n",
    "    print(f\"  Clases de variante (variant): {num_variant_classes}\")\n",
    "    print(f\"  Clases de estilo de mano (hand): {num_hand_classes}\")\n",
    "    \n",
    "    # CAMBIO: Si tu SlowFastNetwork no tiene parámetros para reducir complejidad,\n",
    "    # usa la versión original\n",
    "    model = SlowFastNetwork(\n",
    "        num_stroke_classes=max(2, num_stroke_classes),\n",
    "        num_variant_classes=max(2, num_variant_classes),\n",
    "        num_hand_classes=max(2, num_hand_classes)\n",
    "    ).to(device)\n",
    "    \n",
    "    # CAMBIO: Usar learning rate más alto para converger más rápido en demo\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # CAMBIO: Forzar 1 epoch\n",
    "    num_epochs = 1\n",
    "    \n",
    "    print(\"\\nIniciando entrenamiento en modo demo...\")\n",
    "    \n",
    "    # Versión simplificada para demo\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                # Usar las claves correctas del batch\n",
    "                slow_pathway = batch['slow_pathway'].to(device)\n",
    "                fast_pathway = batch['fast_pathway'].to(device)\n",
    "                \n",
    "                # CAMBIO: Verificar tamaños de tensores\n",
    "                print(f\"Batch {batch_idx}:\")\n",
    "                print(f\"  slow_pathway shape: {slow_pathway.shape}\")\n",
    "                print(f\"  fast_pathway shape: {fast_pathway.shape}\")\n",
    "                \n",
    "                # Pasar los datos por el modelo\n",
    "                stroke_preds, variant_preds, hand_preds = model(slow_pathway, fast_pathway)\n",
    "                \n",
    "                # Mostrar formas de las salidas\n",
    "                print(\"✅ Salida del modelo (modo demo):\")\n",
    "                print(f\"  stroke_preds shape: {stroke_preds.shape}\")\n",
    "                print(f\"  variant_preds shape: {variant_preds.shape}\")\n",
    "                print(f\"  hand_preds shape: {hand_preds.shape}\")\n",
    "                \n",
    "                # Solo procesar 2 batches y salir\n",
    "                if batch_idx >= 1:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error en batch {batch_idx}: {str(e)}\")\n",
    "                print(f\"Claves en el batch: {list(batch.keys())}\")\n",
    "                # Si hay error, mostrar las claves disponibles en el batch\n",
    "                # e intenta usar diferentes nombres si los anteriores no funcionan\n",
    "                for key in batch:\n",
    "                    print(f\"  {key} shape: {batch[key].shape}\")\n",
    "                break\n",
    "    \n",
    "    print(\"\\n✅ Modo demo completado. El modelo se ejecutó correctamente con datos sintéticos.\")\n",
    "    \n",
    "    # CAMBIO: No guardar ningún modelo o checkpoint\n",
    "    print(\"No se guardaron checkpoints en modo demo para ahorrar recursos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837df2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
